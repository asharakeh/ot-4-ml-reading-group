{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/asharakeh/ot-4-ml-reading-group/blob/main/jumbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwfTnwynjm9c",
    "outputId": "0bc8220d-7438-466a-cf35-8a8afdd950cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: POT==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from POT==0.7.0) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from POT==0.7.0) (1.4.1)\n",
      "Requirement already satisfied: cython>=0.23 in /usr/local/lib/python3.7/dist-packages (from POT==0.7.0) (0.29.26)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dependances : \n",
    "- python (3.8.0)\n",
    "- numpy (1.19.2)\n",
    "- torch (1.7.1)\n",
    "- POT (0.7.0)\n",
    "- Cuda\n",
    "\n",
    "command:\n",
    "python3 train.py\n",
    "\n",
    "Author : Kilian Fatras (kilian.fatras@mila.quebec)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "\n",
    "!pip install POT==0.7.0\n",
    "import ot\n",
    "\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXDUFfnquuD2"
   },
   "source": [
    "# Some util functions to evaluate the networks or to make stratified source minibatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWj1pQv1j8o_"
   },
   "outputs": [],
   "source": [
    "#-------- Eval function --------\n",
    "\n",
    "def model_eval(dataloader, model_g, model_f):\n",
    "    \"\"\"\n",
    "    Model evaluation function\n",
    "    args:\n",
    "    - dataloader : considered dataset\n",
    "    - model_g : feature exctrator (torch.nn)\n",
    "    - model_f : classification layer (torch.nn)\n",
    "    \"\"\"\n",
    "    model_g.eval()\n",
    "    model_f.eval()\n",
    "    total_samples =0\n",
    "    correct_prediction = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img = img.to(device)\n",
    "            label = label.long().to(device)\n",
    "            gen_output = model_g(img)\n",
    "            pred = F.softmax(model_f(gen_output), 1)\n",
    "            correct_prediction += torch.sum(torch.argmax(pred,1)==label)\n",
    "            total_samples += pred.size(0)\n",
    "        accuracy = correct_prediction.cpu().data.numpy()/total_samples\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "#--------SAMPLER-------\n",
    "\n",
    "class BalancedBatchSampler(torch.utils.data.sampler.BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_samples for each of the n_classes.\n",
    "    Returns batches of size n_classes * (batch_size // n_classes)\n",
    "    Taken from https://github.com/criteo-research/pytorch-ada/blob/master/adalib/ada/datasets/sampler.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, batch_size):\n",
    "        classes = sorted(set(labels.numpy()))\n",
    "        print(classes)\n",
    "\n",
    "        n_classes = len(classes)\n",
    "        self._n_samples = batch_size // n_classes\n",
    "        if self._n_samples == 0:\n",
    "            raise ValueError(\n",
    "                f\"batch_size should be bigger than the number of classes, got {batch_size}\"\n",
    "            )\n",
    "\n",
    "        self._class_iters = [\n",
    "            InfiniteSliceIterator(np.where(labels == class_)[0], class_=class_)\n",
    "            for class_ in classes\n",
    "        ]\n",
    "\n",
    "        batch_size = self._n_samples * n_classes\n",
    "        self.n_dataset = len(labels)\n",
    "        self._n_batches = self.n_dataset // batch_size\n",
    "        if self._n_batches == 0:\n",
    "            raise ValueError(\n",
    "                f\"Dataset is not big enough to generate batches with size {batch_size}\"\n",
    "            )\n",
    "        print(\"K=\", n_classes, \"nk=\", self._n_samples)\n",
    "        print(\"Batch size = \", batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self._n_batches):\n",
    "            indices = []\n",
    "            for class_iter in self._class_iters:\n",
    "                indices.extend(class_iter.get(self._n_samples))\n",
    "            np.random.shuffle(indices)\n",
    "            yield indices\n",
    "\n",
    "        for class_iter in self._class_iters:\n",
    "            class_iter.reset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_batches\n",
    "    \n",
    "    \n",
    "class InfiniteSliceIterator:\n",
    "    def __init__(self, array, class_):\n",
    "        assert type(array) is np.ndarray\n",
    "        self.array = array\n",
    "        self.i = 0\n",
    "        self.class_ = class_\n",
    "\n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "\n",
    "    def get(self, n):\n",
    "        len_ = len(self.array)\n",
    "        # not enough element in 'array'\n",
    "        if len_ < n:\n",
    "            print(f\"there are really few items in class {self.class_}\")\n",
    "            self.reset()\n",
    "            np.random.shuffle(self.array)\n",
    "            mul = n // len_\n",
    "            rest = n - mul * len_\n",
    "            return np.concatenate((np.tile(self.array, mul), self.array[:rest]))\n",
    "\n",
    "        # not enough element in array's tail\n",
    "        if len_ - self.i < n:\n",
    "            self.reset()\n",
    "\n",
    "        if self.i == 0:\n",
    "            np.random.shuffle(self.array)\n",
    "        i = self.i\n",
    "        self.i += n\n",
    "        return self.array[i : self.i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wjWW7J-uyxe"
   },
   "source": [
    "# Define the models we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDLnGLWFkBmp"
   },
   "outputs": [],
   "source": [
    "   \n",
    "    \n",
    "class Classifier2(nn.Module):\n",
    "    ''' Classifier class'''\n",
    "    def __init__(self, nclass=None):\n",
    "        super(Classifier2, self).__init__()\n",
    "        assert nclass!=None\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def weights_init(m):\n",
    "    ''' Weight init function for layers '''\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.1)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "def call_bn(bn, x):\n",
    "    ''' call batch norm layer '''\n",
    "    return bn(x)\n",
    "\n",
    "\n",
    "class Cnn_generator(nn.Module):\n",
    "    '''9 layer CNN feature extractor class'''\n",
    "    def __init__(self, input_channel=1, n_outputs=10, dropout_rate=0.25, momentum=0.1):\n",
    "        self.momentum = momentum \n",
    "        super(Cnn_generator, self).__init__()\n",
    "        self.c1=nn.Conv2d(input_channel, 32,kernel_size=3, stride=1, padding=1)        \n",
    "        self.c2=nn.Conv2d(32,32,kernel_size=3, stride=1, padding=1)        \n",
    "        self.c3=nn.Conv2d(32,64,kernel_size=3, stride=1, padding=1)        \n",
    "        self.c4=nn.Conv2d(64,64,kernel_size=3, stride=1, padding=1)        \n",
    "        self.c5=nn.Conv2d(64,128,kernel_size=3, stride=1, padding=1)        \n",
    "        self.c6=nn.Conv2d(128,128,kernel_size=3, stride=1, padding=1)        \n",
    "        self.linear1=nn.Linear(128*3*3, 128)\n",
    "        self.bn1=nn.BatchNorm2d(32)\n",
    "        self.bn2=nn.BatchNorm2d(32)\n",
    "        self.bn3=nn.BatchNorm2d(64)\n",
    "        self.bn4=nn.BatchNorm2d(64)\n",
    "        self.bn5=nn.BatchNorm2d(128)\n",
    "        self.bn6=nn.BatchNorm2d(128)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h=x\n",
    "        h=self.c1(h)\n",
    "        h=F.relu(call_bn(self.bn1, h))\n",
    "        h=self.c2(h)\n",
    "        h=F.relu(call_bn(self.bn2, h))\n",
    "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h=self.c3(h)\n",
    "        h=F.relu(call_bn(self.bn3, h))\n",
    "        h=self.c4(h)\n",
    "        h=F.relu(call_bn(self.bn4, h))\n",
    "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h=self.c5(h)\n",
    "        h=F.relu(call_bn(self.bn5, h))\n",
    "        h=self.c6(h)\n",
    "        h=F.relu(call_bn(self.bn6, h))\n",
    "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h = h.view(h.size(0), -1)\n",
    "        logit=torch.sigmoid(self.linear1(h))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KweDmmzku6VB"
   },
   "source": [
    "# Code the Jumbot and source only methods. Fill the blank !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4so8-D-kMYK"
   },
   "outputs": [],
   "source": [
    "class Jumbot(object):\n",
    "    \"\"\"Jumbot class\"\"\"\n",
    "    def __init__(self, model_g, model_f, n_class, eta1=0.001, eta2=0.0001, tau=1., epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Initialize jumbot method.\n",
    "        args :\n",
    "        - model_g : feature exctrator (torch.nn)\n",
    "        - model_f : classification layer (torch.nn)\n",
    "        - n_class : number of classes (int)\n",
    "        - eta_1 : feature comparison coefficient (float)\n",
    "        - eta_2 : label comparison coefficient (float)\n",
    "        - tau : marginal coeffidient (float)\n",
    "        - epsilon : entropic regularization (float)\n",
    "        \"\"\"\n",
    "        self.model_g = model_g   # target model\n",
    "        self.model_f = model_f\n",
    "        self.n_class = n_class\n",
    "        self.eta1 = eta1  # weight for the alpha term\n",
    "        self.eta2 = eta2 # weight for target classification\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon\n",
    "        print('eta1, eta2, tau, epsilon: ', self.eta1, self.eta2, self.tau, self.epsilon)\n",
    "    \n",
    "    def fit(self, source_loader, target_loader, test_loader, n_epochs, criterion=nn.CrossEntropyLoss()):\n",
    "        \"\"\"\n",
    "        Run jumbot method.\n",
    "        args :\n",
    "        - source_loader : source dataset \n",
    "        - target_loader : target dataset\n",
    "        - test_loader : test dataset\n",
    "        - n_epochs : number of epochs (int)\n",
    "        - criterion : source loss (nn)\n",
    "        \n",
    "        return:\n",
    "        - trained model\n",
    "        \"\"\"\n",
    "        target_loader_cycle = itertools.cycle(target_loader)\n",
    "        optimizer_g = torch.optim.Adam(self.model_g.parameters(), lr=2e-4)\n",
    "        optimizer_f = torch.optim.Adam(self.model_f.parameters(), lr=2e-4)\n",
    "\n",
    "        for id_epoch in range(n_epochs):\n",
    "            self.model_g.train()\n",
    "            self.model_f.train()\n",
    "            for i, data in enumerate(source_loader):\n",
    "                ### Load data\n",
    "                xs_mb, ys = data\n",
    "                xs_mb, ys = xs_mb.cuda(), ys.cuda()\n",
    "                xt_mb, _ = next(target_loader_cycle)\n",
    "                xt_mb = xt_mb.cuda()\n",
    "                \n",
    "                g_xs_mb = self.model_g(xs_mb.cuda())\n",
    "                f_g_xs_mb = self.model_f(g_xs_mb)\n",
    "                g_xt_mb = self.model_g(xt_mb.cuda())\n",
    "                f_g_xt_mb = self.model_f(g_xt_mb)\n",
    "                pred_xt = F.softmax(f_g_xt_mb, 1)\n",
    "\n",
    "                ### loss\n",
    "                s_loss = criterion(f_g_xs_mb, ys.cuda())\n",
    "\n",
    "                ###  Ground cost\n",
    "                #embed_cost = \n",
    "                \n",
    "                ys = F.one_hot(ys, num_classes=self.n_class).float()\n",
    "                #t_cost = ??\n",
    "                \n",
    "                #total_cost = ??\n",
    "\n",
    "                #OT computation\n",
    "                #pi = ??\n",
    "                pi = torch.from_numpy(pi).float().cuda()\n",
    "\n",
    "                # train the model \n",
    "                optimizer_g.zero_grad()\n",
    "                optimizer_f.zero_grad()\n",
    "\n",
    "                #da_loss = \n",
    "                tot_loss = s_loss + da_loss\n",
    "                tot_loss.backward()\n",
    "\n",
    "                optimizer_g.step()\n",
    "                optimizer_f.step()\n",
    "            \n",
    "            print('epoch, loss : ', id_epoch, s_loss.item(), da_loss.item())\n",
    "            if id_epoch%10 == 0:\n",
    "                source_acc = self.evaluate(source_loader)\n",
    "                target_acc = self.evaluate(test_loader)\n",
    "                print('source and test accuracies : ', source_acc, target_acc)\n",
    "        \n",
    "        return tot_loss\n",
    "\n",
    "    def source_only(self, source_loader, criterion=nn.CrossEntropyLoss(), n_epochs=10):\n",
    "        \"\"\"\n",
    "        Run source only.\n",
    "        args :\n",
    "        - source_loader : source dataset \n",
    "        - criterion : source loss (nn)\n",
    "        \n",
    "        return:\n",
    "        - trained model\n",
    "        \"\"\"\n",
    "        optimizer_g = torch.optim.Adam(self.model_g.parameters(), lr=2e-4)\n",
    "        optimizer_f = torch.optim.Adam(self.model_f.parameters(), lr=2e-4)\n",
    "\n",
    "        for id_epoch in range(n_epochs):\n",
    "            self.model_g.train()\n",
    "            self.model_f.train()\n",
    "            for i, data in enumerate(source_loader):\n",
    "                ### Load data\n",
    "                xs_mb, ys = data\n",
    "                xs_mb, ys = xs_mb.cuda(), ys.cuda()\n",
    "                \n",
    "                g_xs_mb = self.model_g(xs_mb.cuda())\n",
    "                f_g_xs_mb = self.model_f(g_xs_mb)\n",
    "\n",
    "                ### loss\n",
    "                s_loss = criterion(f_g_xs_mb, ys.cuda())\n",
    "\n",
    "                # train the model \n",
    "                optimizer_g.zero_grad()\n",
    "                optimizer_f.zero_grad()\n",
    "\n",
    "                tot_loss = s_loss\n",
    "                tot_loss.backward()\n",
    "\n",
    "                optimizer_g.step()\n",
    "                optimizer_f.step()\n",
    "        \n",
    "        return tot_loss\n",
    "    \n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        score = model_eval(data_loader, self.model_g, self.model_f)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oie8Uy9cvCvv"
   },
   "source": [
    "# Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zvu2uIRkQ2M",
    "outputId": "cdfac6e9-f90c-4238-d46f-5cb5c580d291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb source data :  7291\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "K= 10 nk= 50\n",
      "Batch size =  500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "nclass = 10\n",
    "np.random.seed(1980)\n",
    "\n",
    "# pre-processing to tensor, and mean subtraction\n",
    "\n",
    "\n",
    "######DATASETS\n",
    "### TRAIN sets\n",
    "transform_usps = transforms.Compose([\n",
    "                    transforms.Resize(28),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                ])\n",
    "\n",
    "train_usps_trainset = datasets.USPS('./data', train=True, download=True,\n",
    "                                transform=transform_usps)\n",
    "\n",
    "print('nb source data : ', len(train_usps_trainset))\n",
    "\n",
    "source_data = torch.zeros((len(train_usps_trainset), 1, 28, 28))\n",
    "source_labels = torch.zeros((len(train_usps_trainset)))\n",
    "\n",
    "for i, data in enumerate(train_usps_trainset):\n",
    "    source_data[i] = data[0]\n",
    "    source_labels[i] = data[1]\n",
    "\n",
    "train_batch_sampler = BalancedBatchSampler(source_labels, batch_size=batch_size)\n",
    "train_usps_loader = torch.utils.data.DataLoader(train_usps_trainset, batch_sampler=train_batch_sampler)\n",
    "\n",
    "transform_mnist = transforms.Compose([\n",
    "                    transforms.Resize(28),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5), (0.5))\n",
    "                ])\n",
    "\n",
    "train_mnist_trainset = datasets.MNIST('./data', train=True, download=True,\n",
    "                                    transform=transform_mnist)\n",
    "train_mnist_loader = torch.utils.data.DataLoader(train_mnist_trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "### TEST sets\n",
    "\n",
    "test_usps_loader = torch.utils.data.DataLoader(\n",
    "        datasets.USPS('./data', train=False, transform=transform_usps, download=True),\n",
    "        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_mnist_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, download=True, transform=transform_mnist),\n",
    "        batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM2K2umnvQKX"
   },
   "source": [
    "# Train the network on the source only and compare the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDM_hvNKmJot",
    "outputId": "b642de2e-5063-4170-9757-a2e8ab2dda76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta1, eta2, tau, epsilon:  0.1 0.1 1.0 0.1\n",
      "source_acc = 0.9735924265072247, target_acc =0.2182\n"
     ]
    }
   ],
   "source": [
    "eta1 = 0.1\n",
    "eta2 = 0.1\n",
    "tau = 1.0\n",
    "epsilon = 0.1\n",
    "\n",
    "model_g = Cnn_generator().cuda().apply(weights_init)\n",
    "model_f = Classifier2(nclass=nclass).cuda().apply(weights_init)\n",
    "\n",
    "model_g.train()\n",
    "model_f.train()\n",
    "\n",
    "jumbot = Jumbot(model_g, model_f, n_class=nclass, eta1=eta1, eta2=eta2, tau=tau, epsilon=epsilon)\n",
    "loss = jumbot.source_only(train_usps_loader, n_epochs=50)\n",
    "\n",
    "source_acc =jumbot.evaluate(test_usps_loader)\n",
    "target_acc =jumbot.evaluate(test_mnist_loader)\n",
    "\n",
    "print(\"source_acc = {}, target_acc ={}\".format(source_acc, target_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0jnqFwyvXDZ"
   },
   "source": [
    "# Train the network with JUMBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keZHotkwq3kn",
    "outputId": "09195065-a200-4d7f-c2e6-97f61143c822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta1, eta2, tau, epsilon:  0.1 0.1 1.0 0.1\n",
      "epoch, loss :  0 0.15685325860977173 0.6192663908004761\n",
      "source and test accuracies :  0.9775714285714285 0.8857\n",
      "epoch, loss :  1 0.11291591078042984 0.40434613823890686\n",
      "epoch, loss :  2 0.10371321439743042 0.28302329778671265\n",
      "epoch, loss :  3 0.07542509585618973 0.22587323188781738\n",
      "epoch, loss :  4 0.06522281467914581 0.20267876982688904\n",
      "epoch, loss :  5 0.05788493528962135 0.15609566867351532\n",
      "epoch, loss :  6 0.0433298796415329 0.1373007893562317\n",
      "epoch, loss :  7 0.04535941779613495 0.1276775747537613\n",
      "epoch, loss :  8 0.03422129154205322 0.10809904336929321\n",
      "epoch, loss :  9 0.030514037236571312 0.09756715595722198\n",
      "epoch, loss :  10 0.033875081688165665 0.09941273182630539\n",
      "source and test accuracies :  0.968 0.9629\n",
      "epoch, loss :  11 0.03565165773034096 0.11504471302032471\n",
      "epoch, loss :  12 0.022952986881136894 0.0845676138997078\n",
      "epoch, loss :  13 0.01983393356204033 0.08031203597784042\n",
      "epoch, loss :  14 0.023547692224383354 0.0669655129313469\n",
      "epoch, loss :  15 0.026097910478711128 0.08303273469209671\n",
      "epoch, loss :  16 0.01977931335568428 0.07153329253196716\n",
      "epoch, loss :  17 0.0189906544983387 0.079039067029953\n",
      "epoch, loss :  18 0.015050502493977547 0.07031115144491196\n",
      "epoch, loss :  19 0.02460423670709133 0.06626486778259277\n",
      "epoch, loss :  20 0.02553308755159378 0.06147610396146774\n",
      "source and test accuracies :  0.9794285714285714 0.9647\n",
      "epoch, loss :  21 0.016425637528300285 0.058937087655067444\n",
      "epoch, loss :  22 0.021950777620077133 0.05870252847671509\n",
      "epoch, loss :  23 0.010352274402976036 0.051278386265039444\n",
      "epoch, loss :  24 0.0103607177734375 0.03942147269845009\n",
      "epoch, loss :  25 0.017440930008888245 0.04949481040239334\n",
      "epoch, loss :  26 0.009953241795301437 0.04901226609945297\n",
      "epoch, loss :  27 0.01675318367779255 0.05465538799762726\n",
      "epoch, loss :  28 0.022570103406906128 0.03933320194482803\n",
      "epoch, loss :  29 0.026215810328722 0.05275005102157593\n",
      "epoch, loss :  30 0.007128589320927858 0.04567304253578186\n",
      "source and test accuracies :  0.9435714285714286 0.976\n",
      "epoch, loss :  31 0.006744049955159426 0.04193533584475517\n",
      "epoch, loss :  32 0.00634830491617322 0.03616585209965706\n",
      "epoch, loss :  33 0.007172893267124891 0.03517509996891022\n",
      "epoch, loss :  34 0.0074491603299975395 0.03641711175441742\n",
      "epoch, loss :  35 0.006129654590040445 0.04079762473702431\n",
      "epoch, loss :  36 0.005522174295037985 0.026924841105937958\n",
      "epoch, loss :  37 0.005630426574498415 0.03019338846206665\n",
      "epoch, loss :  38 0.005131800193339586 0.03246206045150757\n",
      "epoch, loss :  39 0.005933206994086504 0.027271762490272522\n",
      "epoch, loss :  40 0.004885734990239143 0.031544364988803864\n",
      "source and test accuracies :  0.966 0.9633\n",
      "epoch, loss :  41 0.0045921653509140015 0.022814171388745308\n",
      "epoch, loss :  42 0.004345021676272154 0.021925272420048714\n",
      "epoch, loss :  43 0.00479067862033844 0.021701447665691376\n",
      "epoch, loss :  44 0.004180045332759619 0.028959596529603004\n",
      "epoch, loss :  45 0.00751994363963604 0.025226490572094917\n",
      "epoch, loss :  46 0.003990740515291691 0.02231932431459427\n",
      "epoch, loss :  47 0.0038986129220575094 0.018151119351387024\n",
      "epoch, loss :  48 0.00369053753092885 0.01995491050183773\n",
      "epoch, loss :  49 0.0037168466951698065 0.030759476125240326\n",
      "epoch, loss :  50 0.0043144673109054565 0.025938797742128372\n",
      "source and test accuracies :  0.941 0.9702\n",
      "epoch, loss :  51 0.003422294743359089 0.019843436777591705\n",
      "epoch, loss :  52 0.0035250987857580185 0.023904509842395782\n",
      "epoch, loss :  53 0.0032525560818612576 0.018200673162937164\n",
      "epoch, loss :  54 0.017920762300491333 0.028374778106808662\n",
      "epoch, loss :  55 0.003142405766993761 0.024281803518533707\n",
      "epoch, loss :  56 0.003038643626496196 0.024827778339385986\n",
      "epoch, loss :  57 0.003010156797245145 0.015372558496892452\n",
      "epoch, loss :  58 0.002886079950258136 0.01606576517224312\n",
      "epoch, loss :  59 0.0028790452051907778 0.020023852586746216\n",
      "epoch, loss :  60 0.0027539748698472977 0.016180962324142456\n",
      "source and test accuracies :  0.9632857142857143 0.9739\n",
      "epoch, loss :  61 0.0027382601983845234 0.015997115522623062\n",
      "epoch, loss :  62 0.002590964315459132 0.011370232328772545\n",
      "epoch, loss :  63 0.0026263713371008635 0.014406528323888779\n",
      "epoch, loss :  64 0.0024907533079385757 0.012616630643606186\n",
      "epoch, loss :  65 0.002450843807309866 0.017106063663959503\n",
      "epoch, loss :  66 0.0024127515498548746 0.013643302954733372\n",
      "epoch, loss :  67 0.0023738816380500793 0.01251547783613205\n",
      "epoch, loss :  68 0.0022873361594974995 0.011064634658396244\n",
      "epoch, loss :  69 0.0022753390949219465 0.01085951179265976\n",
      "epoch, loss :  70 0.002235231688246131 0.01045367494225502\n",
      "source and test accuracies :  0.9814285714285714 0.9723\n",
      "epoch, loss :  71 0.0021652569994330406 0.01502499170601368\n",
      "epoch, loss :  72 0.0021281344816088676 0.012674238532781601\n",
      "epoch, loss :  73 0.002128572203218937 0.01397661492228508\n",
      "epoch, loss :  74 0.0020870885346084833 0.01285543106496334\n",
      "epoch, loss :  75 0.0020044485572725534 0.01794123835861683\n",
      "epoch, loss :  76 0.001960309222340584 0.013933716341853142\n",
      "epoch, loss :  77 0.0019416363211348653 0.011444013565778732\n",
      "epoch, loss :  78 0.001889657462015748 0.01611735299229622\n",
      "epoch, loss :  79 0.0018756769131869078 0.01294476818293333\n",
      "epoch, loss :  80 0.0018561213510110974 0.01339695043861866\n",
      "source and test accuracies :  0.9774285714285714 0.9656\n",
      "epoch, loss :  81 0.0018363448325544596 0.013934284448623657\n",
      "epoch, loss :  82 0.0017842416418716311 0.0175224170088768\n",
      "epoch, loss :  83 0.0017725919606164098 0.011905599385499954\n",
      "epoch, loss :  84 0.0017237602733075619 0.011404354125261307\n",
      "epoch, loss :  85 0.0017477968940511346 0.013585135340690613\n",
      "epoch, loss :  86 0.0016657363157719374 0.012542679905891418\n",
      "epoch, loss :  87 0.0016780882142484188 0.015821052715182304\n",
      "epoch, loss :  88 0.0016042127972468734 0.006342931650578976\n",
      "epoch, loss :  89 0.0016020729672163725 0.0160413458943367\n",
      "epoch, loss :  90 0.0015515581471845508 0.01321553997695446\n",
      "source and test accuracies :  0.9635714285714285 0.9758\n",
      "epoch, loss :  91 0.0015235628234222531 0.012587660923600197\n",
      "epoch, loss :  92 0.0015039286809042096 0.012897990643978119\n",
      "epoch, loss :  93 0.0014842039672657847 0.011341575533151627\n",
      "epoch, loss :  94 0.0014627103228121996 0.008136723190546036\n",
      "epoch, loss :  95 0.001431661774404347 0.012055182829499245\n",
      "epoch, loss :  96 0.0014200947480276227 0.009987298399209976\n",
      "epoch, loss :  97 0.016280993819236755 0.01134083978831768\n",
      "epoch, loss :  98 0.016228409484028816 0.007733635604381561\n",
      "epoch, loss :  99 0.0013669525505974889 0.006359809543937445\n",
      "source_acc = 0.9347284504235177, target_acc =0.9786\n"
     ]
    }
   ],
   "source": [
    "model_g = Cnn_generator().cuda().apply(weights_init)\n",
    "model_f = Classifier2(nclass=nclass).cuda().apply(weights_init)\n",
    "\n",
    "model_g.train()\n",
    "model_f.train()\n",
    "\n",
    "jumbot = Jumbot(model_g, model_f, n_class=nclass, eta1=eta1, eta2=eta2, tau=tau, epsilon=epsilon)\n",
    "loss = jumbot.source_only(train_usps_loader)\n",
    "loss = jumbot.fit(train_usps_loader, train_mnist_loader, test_mnist_loader, n_epochs=100)\n",
    "\n",
    "source_acc =jumbot.evaluate(test_usps_loader)\n",
    "target_acc =jumbot.evaluate(test_mnist_loader)\n",
    "\n",
    "print(\"source_acc = {}, target_acc ={}\".format(source_acc, target_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW_7kuPisqg9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP+4wjfzGbilu0wlkMVDx/K",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "jumbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
